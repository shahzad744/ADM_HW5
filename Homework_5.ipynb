{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Group 11\n",
    "\n",
    "In this assignment we perform an analysis of the Wikipedia Hyperlink graph. In particular, given extra information about the categories to which an article belongs to, we are curious to rank the articles according to some criteria.\n",
    "\n",
    "For this purpose we use the Wikipedia graph released by the SNAP group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before starting - Clean the category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the unique nodes\n",
    "\n",
    "We read the file to count and save all the nodes we find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_file = open(\"wiki-topcats-reduced.txt\", \"r\") \n",
    "\n",
    "unique_nodes = set()\n",
    "\n",
    "for edge in nodes_file:\n",
    "    unique_nodes.update( map(int, edge.split()) )\n",
    "\n",
    "nodes_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many nodes we consider?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461193"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a default dict for the categories\n",
    "\n",
    "Then we consider the file with the categories and the nodes that belong to them. However there are some categories that have been removed from the graph. So we select only the categories that have at least one node in the \"unique_node\" set. Another criterion for the choice is taking into account all the categories that have number of articles greater than 3500 in the category file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict = defaultdict()\n",
    "\n",
    "categ_file = open(\"wiki-topcats-categories.txt\", \"r\") \n",
    "\n",
    "for categ in categ_file:\n",
    "    list_row = categ.split(\";\")\n",
    "    category_dict[list_row[0][9:]] = list(map(int,list_row[1].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many categories the file contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17364"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(category_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look for the categories to delete and we store them into a list.\n",
    "\n",
    "We delete each key that has the a number of nodes lower than 3500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_delete = [key for key, val in category_dict.items() if len(val) < 3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17329"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(key_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in key_to_delete:\n",
    "    del(category_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many categories remain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(category_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover we substitute, for each category, the list of original nodes with a list with a new list of nodes of that category that are also in \"unique_nodes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in category_dict.items():\n",
    "    inters = set(val).intersection(unique_nodes)\n",
    "    if  bool( inters ) == True:\n",
    "        category_dict[key] = list(inters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('category_dict.json', 'w') as fp:\n",
    "    json.dump(category_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English_footballers 7538\n",
      "The_Football_League_players 7814\n",
      "Association_football_forwards 5097\n",
      "Association_football_goalkeepers 3737\n",
      "Association_football_midfielders 5827\n",
      "Association_football_defenders 4588\n",
      "Living_people 348300\n",
      "Year_of_birth_unknown 2536\n",
      "Harvard_University_alumni 5549\n",
      "Major_League_Baseball_pitchers 5192\n",
      "Members_of_the_United_Kingdom_Parliament_for_English_constituencies 6491\n",
      "Indian_films 5568\n",
      "Year_of_death_missing 4122\n",
      "English_cricketers 3275\n",
      "Year_of_birth_missing_(living_people) 28498\n",
      "Rivers_of_Romania 7729\n",
      "Main_Belt_asteroids 11660\n",
      "Asteroids_named_for_people 4895\n",
      "English-language_albums 4760\n",
      "English_television_actors 3362\n",
      "British_films 4422\n",
      "English-language_films 22463\n",
      "American_films 15159\n",
      "Fellows_of_the_Royal_Society 3446\n",
      "People_from_New_York_City 4614\n",
      "American_Jews 3411\n",
      "American_television_actors 11531\n",
      "American_film_actors 13865\n",
      "Debut_albums 7561\n",
      "Black-and-white_films 10759\n",
      "Year_of_birth_missing 4346\n",
      "Place_of_birth_missing_(living_people) 5532\n",
      "Article_Feedback_Pilot 3485\n",
      "American_military_personnel_of_World_War_II 3720\n",
      "Windows_games 4025\n"
     ]
    }
   ],
   "source": [
    "for key, val in category_dict.items():\n",
    "    print(key, len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a graph where:\n",
    "- the vertices are the article IDs\n",
    "- the edges are the hyperlinks among them\n",
    "\n",
    "At the end we answer to basic answers io order to give some basic information about the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_file = open(\"wiki-topcats-reduced.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph will have the shape of a dictionary. \n",
    "\n",
    "Reading all the edges, a key will be a source node and its value will be a list of all the destination nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = defaultdict(list)\n",
    "for edge in nodes_file:\n",
    "    l = edge.split()\n",
    "    graph[int(l[0])].append(int(l[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428957"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[401135, 1069112, 1163551]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of item in graph\n",
    "graph[52]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Is the graph direct or not?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Definition*: A graph is said to be direct if, taken two nodes U and V, the arc E(u,v) is different from the arc E(v,u).\n",
    "\n",
    "We must therefore prove that one node is connected to another, but that this one is not connected to the first, to be able to say that the graph is directed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[401135, 1069112, 1163551]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "52 in graph[401135]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have an edge between node 52 and node 401135 but it's not true the viceversa, the graph is certainly directed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 - Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461193"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_nodes = len(unique_nodes)\n",
    "number_of_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 - Number of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2645247"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_edges = 0\n",
    "for val in graph.values():\n",
    "    number_of_edges += len(val)\n",
    "number_of_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 - Average node degree\n",
    "\n",
    "The definition of degree of a vertex of a graph is the number of edges incident to the vertex.\n",
    "\n",
    "To do this and as it will come in handy later, we create a \"reverse\" graph. With the dictionary shape it will have as key a destination node and as value a list of all the nodes from which an edge starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_graph = defaultdict(list)\n",
    "\n",
    "for key,val in graph.items():\n",
    "    for elem in val:\n",
    "        reverse_graph[elem].append(key)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = 0\n",
    "for val in reverse_graph.values():\n",
    "    summ += len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.503863632495362"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_node_degree = summ/len(reverse_graph.keys())\n",
    "avg_node_degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 -  Is the graph dense?\n",
    "\n",
    "Density of oriented graph is defined as:\n",
    "$\\Delta = \\frac{L}{n(n - 1)}$, where n = number of nodes = 461193 and L = number of edges = 2645247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2436602635647606e-05"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = number_of_edges / (number_of_nodes * (number_of_nodes - 1))\n",
    "delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph isn't really dense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Obtain the block-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to choose our C0 category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year_of_birth_unknown\n"
     ]
    }
   ],
   "source": [
    "category = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I create the structure of the dictionary I use with the Bfs function\n",
    "len_path = {}\n",
    "for node in unique_nodes:\n",
    "    len_path[node] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let's implement the BFS algorithm:\n",
    "\n",
    "Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key'), and explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the specific case our algorithm is slightly modified (To better fit our need to have the nodes outside the starting category and as values the shortest path list from all the starting category nodes that can reach the key node considered) and this is the pseudocode:\n",
    "\n",
    "- Let's first consider the parent node and set the current distance equal to 0, I initialize queue as the set of the children of the parent node, and visited as the set of the visited nodes.\n",
    "\n",
    "Until queue has at least one element, that's what we do:\n",
    "\n",
    "- We increase current distance.\n",
    "\n",
    "- For each child we append the node with the current distance as value, I find the children of v and I save them in a list and finally I add the node v to the set of visited nodes.\n",
    "\n",
    "Then we update our queue as the difference between the set of children of v and the set of visited (because It's useless revisit the same node).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bfs(g, s, len_path):\n",
    "    curr_dist = 0 # current distance\n",
    "    len_path[s].append(curr_dist) # the first element is s himself, so I put a 0 distance.\n",
    "    queue = set(graph[s]) # the set of the children of s\n",
    "    visited = set([s]) # the set of the visited items, at the beginning is only s.\n",
    "    \n",
    "    while queue:\n",
    "        \n",
    "        curr_dist += 1 # update the current distance\n",
    "        temp = []\n",
    "        for v in queue: # for each child of s\n",
    "            len_path[v].append(curr_dist) # append the node with distance equal to curr_dist\n",
    "            temp += graph[v] # this is the list of the children of v (child of s)\n",
    "            visited.add(v) # I add in visited the node v\n",
    "            \n",
    "        queue = set(temp).difference(visited) # now queue become the difference between set and visited.\n",
    "        \n",
    "    return len_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each node in the choosen category, I do Bfs\n",
    "for node in category_dict[category]:\n",
    "    len_path = Bfs(graph, node, len_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the dictionary obtained as a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lenpath.pickle', 'wb') as handle:\n",
    "    pickle.dump(len_path, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('lenpath.pickle', 'rb')\n",
    "len_path = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to compute the median to do the block-ranking of the categories respect the starting category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_ranking = defaultdict(list)\n",
    "# number of nodes in requested category\n",
    "cat_nodes = len(category_dict[category])\n",
    "\n",
    "for cat in category_dict.keys():\n",
    "    \n",
    "    # category requested - median 0\n",
    "    if cat == category:\n",
    "        block_ranking[0].append(cat)\n",
    "     \n",
    "    else:\n",
    "        # number of nodes in the category - cat\n",
    "        cat_n = len(category_dict[cat])\n",
    "        # length of the list\n",
    "        tot = cat_n * cat_nodes\n",
    "        # intialize list of all the shortest paths of this category - cat\n",
    "        list_path = []\n",
    "        # iterate all the node in category cat\n",
    "        for node in category_dict[cat]:\n",
    "            \n",
    "            list_path += len_path[node]\n",
    "        \n",
    "        if len(list_path) != tot:\n",
    "            # we add the necessary number of infinites\n",
    "            list_path += ([np.inf] * (tot - len(list_path)))\n",
    "        array_path = np.array(list_path)\n",
    "        \n",
    "        \n",
    "        key = np.median(array_path)\n",
    "        \n",
    "        # update the block ranking \n",
    "        block_ranking[key].append(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('blockranking.pickle', 'wb') as handle:\n",
    "    pickle.dump(block_ranking, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('blockranking.pickle', 'rb')\n",
    "block_ranking = pickle.load(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {inf: ['English_footballers',\n",
       "              'The_Football_League_players',\n",
       "              'Association_football_forwards',\n",
       "              'Association_football_goalkeepers',\n",
       "              'Association_football_midfielders',\n",
       "              'Association_football_defenders',\n",
       "              'Living_people',\n",
       "              'Harvard_University_alumni',\n",
       "              'Major_League_Baseball_pitchers',\n",
       "              'Year_of_death_missing',\n",
       "              'English_cricketers',\n",
       "              'Year_of_birth_missing_(living_people)',\n",
       "              'Main_Belt_asteroids',\n",
       "              'Asteroids_named_for_people',\n",
       "              'Fellows_of_the_Royal_Society',\n",
       "              'Year_of_birth_missing',\n",
       "              'Place_of_birth_missing_(living_people)',\n",
       "              'American_military_personnel_of_World_War_II',\n",
       "              'Windows_games'],\n",
       "             0: ['Year_of_birth_unknown'],\n",
       "             7.0: ['Members_of_the_United_Kingdom_Parliament_for_English_constituencies',\n",
       "              'Indian_films',\n",
       "              'English_television_actors',\n",
       "              'British_films',\n",
       "              'American_films',\n",
       "              'American_Jews',\n",
       "              'American_television_actors',\n",
       "              'Black-and-white_films',\n",
       "              'Article_Feedback_Pilot'],\n",
       "             8.0: ['Rivers_of_Romania',\n",
       "              'English-language_albums',\n",
       "              'People_from_New_York_City'],\n",
       "             6.0: ['English-language_films', 'American_film_actors'],\n",
       "             9.0: ['Debut_albums']})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year_of_birth_unknown',\n",
       " 'English-language_films',\n",
       " 'American_film_actors',\n",
       " 'Members_of_the_United_Kingdom_Parliament_for_English_constituencies',\n",
       " 'Indian_films',\n",
       " 'English_television_actors',\n",
       " 'British_films',\n",
       " 'American_films',\n",
       " 'American_Jews',\n",
       " 'American_television_actors',\n",
       " 'Black-and-white_films',\n",
       " 'Article_Feedback_Pilot',\n",
       " 'Rivers_of_Romania',\n",
       " 'English-language_albums',\n",
       " 'People_from_New_York_City',\n",
       " 'Debut_albums',\n",
       " 'English_footballers',\n",
       " 'The_Football_League_players',\n",
       " 'Association_football_forwards',\n",
       " 'Association_football_goalkeepers',\n",
       " 'Association_football_midfielders',\n",
       " 'Association_football_defenders',\n",
       " 'Living_people',\n",
       " 'Harvard_University_alumni',\n",
       " 'Major_League_Baseball_pitchers',\n",
       " 'Year_of_death_missing',\n",
       " 'English_cricketers',\n",
       " 'Year_of_birth_missing_(living_people)',\n",
       " 'Main_Belt_asteroids',\n",
       " 'Asteroids_named_for_people',\n",
       " 'Fellows_of_the_Royal_Society',\n",
       " 'Year_of_birth_missing',\n",
       " 'Place_of_birth_missing_(living_people)',\n",
       " 'American_military_personnel_of_World_War_II',\n",
       " 'Windows_games']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a list that will contain the categories sorted\n",
    "block_ranking_sorted = []\n",
    "# iterate over the medians sorted...\n",
    "for median in sorted(block_ranking.keys()):\n",
    "    # we concatenate the categories\n",
    "    block_ranking_sorted += block_ranking[median]\n",
    "block_ranking_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 -  Sort the nodes in each category\n",
    "\n",
    "Now we want to sort the categories using the medians obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned category file\n",
    "category_dict = json.load(open('category_dict.json', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want build a function that has in input the list of category sorted and for each category we sort the nodes inside. For each category ( = a list of nodes ) we create a dictionary where each keys is a node and its values the weight. In example:\n",
    "\n",
    "category_0 = [23, 342, 1233, 34, 762]\n",
    "\n",
    "Using the reverse graph we see that the node 23 is the destination of 2 edges of the same category. Then the new dictionary start with:\n",
    "\n",
    "categ_0_weighted = { 23 : 2 }\n",
    "\n",
    "Repeating the procedure for each nodes and we obtain something like this:\n",
    "\n",
    "categ_0_weighted = { 23 : 2, 342 : 0, 1233 : 1, 34 : 0, 762 : 3 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_a_category( categ ):\n",
    "    # initialize a default dict where we store the category with weighted nodes\n",
    "    category_weighted = defaultdict(int)\n",
    "    # the list of nodes in this category\n",
    "    list_of_nodes = category_dict[categ]\n",
    "    # for all the nodes in that category we compute its weight\n",
    "    for one_node in list_of_nodes:\n",
    "        # the KEY is the node and the VALUE is the in-edges of the same category\n",
    "        category_weighted[one_node] = len(set(reverse_graph[one_node]).intersection(set(list_of_nodes)))\n",
    "    # at the end we return the dictionary of the category weighted      \n",
    "    return(category_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want a function that add the weight of the in-edge of the previous category. So we take in input the new category to weight and the previous ( where the order was decided in the block ranking )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_with_previous_categ ( current_categ, previous_categ_weighted ):\n",
    "    # First of all we weight all of nodes according to in-edges of the same category\n",
    "    category_weighted = sort_a_category( current_categ )\n",
    "    # the list of nodes in this category\n",
    "    list_of_nodes = category_dict[current_categ]\n",
    "    # for all the nodes in that category...\n",
    "    for one_node in list_of_nodes:\n",
    "        # we search all the nodes in the previous category that have as a destination \"one_node\".\n",
    "        # And iterating over this...\n",
    "        for neighboor_node in list(set(reverse_graph[one_node]).intersection(set(previous_categ_weighted.keys()))):\n",
    "            # we add to \"one_node\" the weight of the source node\n",
    "            category_weighted[one_node] += previous_categ_weighted[neighboor_node]\n",
    "    # at the end we return the current weighted category\n",
    "    return category_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_the_graph( block_ranked ):\n",
    "    # computed the weight for the first category\n",
    "    previous_cat = sort_a_category(block_ranked[0])\n",
    "    # sort the elements in descending order by values ( = weights of the nodes)\n",
    "    sorted_by_value = sorted(previous_cat.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    # initialize the output list where we will have the nodes ordered by category and weight\n",
    "    output_list = [k for k,v in sorted_by_value]\n",
    "    # we repeat all the process for the following categories after the first\n",
    "    for categ_name in block_ranked[1:]:\n",
    "        previous_cat = sort_with_previous_categ(categ_name, previous_cat)\n",
    "        sorted_by_value = sorted(previous_cat.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        output_list += [k for k,v in sorted_by_value if k not in output_list]\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_articles = sort_the_graph( block_ranking_sorted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('listfile.data', 'wb') as filehandle:  \n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(ordered_articles, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('listfile.data', 'rb') as filehandle:  \n",
    "    # read the data as binary data stream\n",
    "    ordered_articles = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62684, 170163, 1656777, 1656780, 169696, 1656794, 170578, 1342864, 1343014, 1656778, 666855, 1342960, 1779656, 174582, 1109348, 1109485, 159606, 159730, 159920, 1766063, 1203095, 1203235, 64632, 1203496, 1122762, 1344701, 34422, 1443739, 174427, 174439, 166284, 666857, 159750, 168001, 168145, 168251, 168258, 1765824, 1765831, 1765837, 185120, 62695, 1340874, 170158, 1203101, 170969, 170970, 170971, 170972, 170973, 1342803, 1343206, 958480, 172050, 1319048, 360595, 1122603, 1122605, 156307, 173221, 1344730, 1344821, 173671, 1656450, 1656452, 1656453, 1656455, 1443741, 1656779, 1656793, 1779795, 1779800, 174428, 1345946, 1345947, 1190355, 60061, 175366, 1109359, 159614, 167906, 159736, 159749, 159753, 159754, 159766, 167966, 168100, 159914, 159934, 176367, 168194, 1765823, 1765832, 1765845, 1765848, 201333, 1684172, 324414, 1766449]\n"
     ]
    }
   ],
   "source": [
    "# Example of the output with input category C0 = Year_of_birth_unknown\n",
    "print(ordered_articles[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
